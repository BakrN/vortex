# 1 FULL TC Operation

D = A x B + C

## Problem size
Determined by exactly unrolling loops exactly over all the
K  //(determines "sets") (temporally unrolled)
M
N

## Software instruction parameters matrices
Sets // temporal unrolling of K (sets will determine the sets)
A : [M x K] // Comes from GPU register file and loaded into A matrix buffer
B : [K x N] // Comes from GPU register file and loaded into B matrix buffer
C : [M x N] // Can come from GPU register file or from accumulation buffer and loaded into C register in Tensor core
D : [M x N] // Output final result to GPU register file or memory

## Configuration

// Can have a dispatch unit to calculate thread stuff multiple times? NO NO NO NO NO NO NO NO NO ( so what can I actually change) (requirement there will be 1 pe per thread this is my 1 requirement)

Things I can physically


### TC & Problem parameters

num_pe_groups       // Number of PE groups with shared resource
num_pe_per_group    // Number of individual PEs in a group
dot_product_size    // Dot product width
buffer_depth        // Buffer width for A and B operand collectors
num_acc_tile        // Number of accumulation tile I can store
SETS                (determines K along with dot_product_size)

### Constraints & Derived
num_threads = num_pe_groups * num_pe_per_group

A_buffer_width = dot_product_size
B_buffer_width = dot_product_size // If i can come up with a new instruction format then this can be an independent parameter
tileSizeRow = num_pe_per_group
tileSizeCol = B_buffer_width * (num_acc_tile ? num_acc_tile : 1)
steps = B_buffer_width

### HW Physical buffers

mat_tile_accumulate : [tileSizeRow x tileSizeCol x num_acc_tile]
A_buf : [buffer_depth x A_buffer_width ]
B_buf : [buffer_depth x B_buffer_width ]
C_reg : [num_pe_per_group x num_pe_groups] 1 per PE

### Problem size
# can share A between thread groups  (maximum REUSE IMPLEMENT THIS LATER) (Do this later)
# Derivation


tileSizeRow*tileSizeCol=M*N/num_pe_groups

N = num_pe_per_group * dot_product_size * (num_acc_tile ? num_acc_tile : 1) // based on stepping mechanism in design
M = tileSizeRow * tileSizeCol /  dot_product_size * (num_acc_tile ? num_acc_tile : 1) = num_pe_per_group * B_buffer_width /  dot_product_size
K = SETS * dot_product_size // B_buffer_size

# Parfor

```cpp
for (i = 0; i < M; i += tileSizeRow)		// unrolled over threadgroups (A or B can also be shared between threadgroups)
	for (j = 0; j < N*(max(num_acc_tile,1)); j += tileSizeCol)	// unrolled over threadgroups
		acc_tile = j / N;
		for (k = 0; k < K; k += num_threads_per_group) // sets (instructions) (generated by TC controller) (tile_size = num_threads_per group)  (configurable)
			// Loop inside the current tile
			for (ii = i; ii < i + tileSizeRow && ii < M; ii++) { // unrolled over threads within thread group
				for (jj = j; jj < j + tileSizeCol && jj < N; jj++) { // steps  (Means B[kk][jj] is shared between threads within a threadgroup)
					if (acc_buffer_enabled) {
						C[ii][jj] = mat_tile_accumulate[acc_tile][ii][jj]
					}
					for (kk = k ; kk < dot_step + dot_size && kk < K; kk++) {
						for (dot_step = kk ; dot_step < dot_step+dot_size && dot_step < K; dot_step++)) // unrolled over inner product (dot product uint)
							temp[ii][jj] += A[ii][kk] * B[kk][jj] + C[ii][jj];                      // This temp is stored in accumulation buffer (temp can either be mat_tile_accumulate or D depending on which flag I set)
							mat_tile_accumulate[acc_tile][ii][jj] += temp[ii][jj];
		for (acc_tile = 0 ; i < num; acc_tile++)
			D[i...][j...] = mat_tile_accumulate[acc_tile][i...][j...]; // assign block (store)
```
# TO BE IMPLEMENTED IDEAS

Support for partion thread masks. -> Idea is share physical compute units over threads...
Thread mask. In simx, need to when commit have tmask on only commit once all ouutput fifos are full...

# Appendix
(how to avoid conflict when loading from different warps? Have a TC Controller accept it) ( I will need a TC controller if I want multiple waves to overlap) (setup latency okay as long as it doesn't take To be in steady state, loading_latency < steps

(important behaviour and not in steady state)


- If all PE Groups are operating synchronously then I will be able to have more data reuse (even A between PE Groups) How will driver strength affect timing behaviour (very low level)

